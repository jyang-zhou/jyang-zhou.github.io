---
title: Clustering Method
commentable: flase
Edit: 2021-12-28
mathjax: true
mermaid: true
tags: Clustering
categories: Data
description: Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields. In this project, k-means cluster, hierarchical cluster, and spectral cluster are provided in the sample code.
---

# Clustering-Method
Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields. In this project, k-means cluster, hierarchical cluster, and spectral cluster are provided in the sample code.

## Formulation for k-mean:

The input of the algorithm is the raw data, number of clusters, number of time the k-means algorithm will be run with different centroid seeds, and max iterations.

In K-mean, what we want to do is to minimize the "distortion measure" $J$, where

$$J=\sum_{n=1}^N\sum_{k=1}^Nr_{nk}||x_n-\mu_k||$$,

which represents the sum of the squares of the distances of each data point to its assigned vector. 

In order to minimize $J$, we have to find values for $r_{nk}$ and $\mu_k$. And we assign 

$$
r_{nk}=
\left\{  
             \begin{array}{**lr**}  
             1, \qquad k = argmin_j||x_n-\mu_j|| \\
             0, \qquad otherwise
             \end{array}  
\right. 
$$
So now by finding the derivative of $J$ with respect to $\mu_k$, we will have

$$2\sum_{n=1}^N r_{nk}(x_n-\mu_k)=0$$

and $\mu_k$ is given by $$\mu_k=\frac {\sum_n r_{nk}x_n} {\sum_n r_{nk}}$$



## Formulation for hierarchical cluster

The input of the algorithm is the raw data, number of clusters, method to compute distances between clusters, and methods to compute distance matrix.

I will show the algorithm of hierarchical cluster using Ward's method.

1. Start with each point in a cluster by itself (sum of squares = 0).

2. Merge two clusters, in order to produce the smallest increase in the sum of squares (the smallest merging cost).

3. Keep merging until you’ve reached k clusters.

Let $X_{ijk}$ denote the value for variable $k$ in observation $j$ belonging to cluster $i$.

Ward's method wants to maximize $r^2$ in each step where
$$r^2=\frac {TSS-ESS} {TSS}$$
such that $TSS=\sum_i \sum_j \sum_k |X_{ijk}-\hat{x}_{..k}|^2$, and $ESS=\sum_i \sum_j \sum_k |X_{ijk}-\hat{x}_{i.k}|^2$


## Formulation for Spectral cluster

The input of the algorithm is the raw data, number of clusters, number of neighbors, and gamma.

Spectral clustering is get a similarity matrix S, and then create a graph $G=(V,E)$ such that one node $v_i$ for each $x_i$ and edge weights $w_{ij}$ based on $s_{ij}$.

Now we define $W$ be the similarity matrix, $D$ be the diagonal degree matrix $D_{ii}=\sum_jW_{ij}$. And we call $L=D-W$ the unnormalized graph Laplacian. 

In order to get the cluster, the goal has been changed into minimize Ncut
$$min_x Ncut(x)=min_y\frac{y^TLy} {y^TDy}$$\
$$s.t.\qquad y^TDy=1$$

And now we get $Ly=\lambda Dy$.

In order to get $W$ AND $D$, we define pixel content $F(i)$ and pixel location $X(i)$. So $w_{ij}$ is now
$$
w_{ij}=
\left\{  
             \begin{array}{**lr**}  
             e^{-(\frac {||F(i)-F(j)||^2} {2\pi\rho^2} +\frac {||X(i)-X(j)||^2} {2\pi\tau^2})}, \qquad ||X(i)-X(j)||< r \\
             0, \qquad otherwise
             \end{array}  
\right. 
$$

By getting $W$, we will know $D$ and calculate the corresponding cluster $y$ given the input data.


# Dataset

![image](https://user-images.githubusercontent.com/95513386/146653351-a4be3334-4b56-4b1b-b76d-bdf26f6d49d0.png)


We crawled articles in different areas of Wikipedia. The topics of these articles form a hierarchy (given by Wikipedia), as shown
in Figure 1. We selected 10 documents in each of the finest topic categories (leaf nodes in the figure). We
provide you the documents in a featurized form.

Each document has undergone the following pre-proessing steps:
1. Removal of links, formulas, tables, reference lists, and media other than text.
2. Removal of certain words. These words include “stop words” that do not convey significant meaning,
such as “a”, “the”, “me”, and digits (for purpose of controlling the vocabulary size).
3. Stemming, where the affixes of each word are removed and only the remaining stem is retained. The
vocabulary size (number of unique word types) in the dataset is 9,729 after pre-processing.

# Visualization and Accuracy
## Hierarchical Clustering
NMI: 0.608, Purity: 0.500

![image](https://user-images.githubusercontent.com/95513386/146653792-f55af265-f9af-4d23-9859-1e61ad694c28.png)

![image](https://user-images.githubusercontent.com/95513386/146653979-34b1a3d9-ff9f-41cb-bdec-06fcb2f74cee.png)

## K-means Cluster
NMI: 0.514, Purity: 0.425

![image](https://user-images.githubusercontent.com/95513386/146653996-2358262e-e104-4616-8fb3-9cc0e3fb382c.png)

## Spectral Cluster
NMI: 0.689, Purity: 0.581

![image](https://user-images.githubusercontent.com/95513386/146654016-8efdd568-047d-4db5-a800-de2a7af098a8.png)
